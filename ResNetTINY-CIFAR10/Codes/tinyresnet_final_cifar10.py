# -*- coding: utf-8 -*-
"""TinyResNet_Final-CIFAR10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q-PCx8OhbdYiCbPc5TaJyjRjvKB0wGiI

# Download packages
"""

!pip install git+https://github.com/qubvel/classification_models.git

!pip install tensorflow-addons

"""# Initialisation and data cleaning"""

# Import resnet lib
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, Add, Flatten, Dense, GlobalAveragePooling2D, MaxPooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.applications.resnet import ResNet50
import matplotlib.pyplot as plt
from tensorflow.keras.optimizers import Adam, SGD, Nadam
import time
from classification_models.keras import Classifiers
import tensorflow_addons as tfa
from keras import backend as K

import subprocess
print(subprocess.getoutput('nvidia-smi'))

(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# Normalize pixel values to be between 0 and 1
x_train, x_test = x_train / 255.0, x_test / 255.0

# Convert class vectors to binary class matrices
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

"""# Build Tiny ResNet and padam"""

def resnet_block(x, filters, kernel_size=3, stride=1, conv_shortcut=True, name=None):
    residual = x

    if conv_shortcut:
        residual = Conv2D(filters, 1, strides=stride, padding='same', name=name+'_shortcut')(x)
        residual = BatchNormalization(name=name+'_shortcut_bn')(residual)

    x = BatchNormalization(name=name+'_bn1')(x)
    x = Activation('relu', name=name+'_relu1')(x)
    x = Conv2D(filters, kernel_size, padding='same', name=name+'_conv1')(x)

    x = BatchNormalization(name=name+'_bn2')(x)
    x = Activation('relu', name=name+'_relu2')(x)
    x = Conv2D(filters, kernel_size, strides=stride, padding='same', name=name+'_conv2')(x)

    x = Add(name=name+'_add')([x, residual])
    return x

# Create a tiny resnet to demand the limited computing performance
def create_tiny_resnet(input_shape, num_classes):
    inputs = Input(shape=input_shape)
    x = Conv2D(16, 3, padding='same', activation='relu', name='conv1')(inputs)

    # Only 2 residual blocks instead of 3 for each stage
    x = resnet_block(x, 16, name='block1_0')
    x = resnet_block(x, 16, name='block1_1')

    x = resnet_block(x, 32, stride=2, name='block2_0')
    x = resnet_block(x, 32, name='block2_1')

    x = resnet_block(x, 64, stride=2, name='block3_0')
    x = resnet_block(x, 64, name='block3_1')

    x = BatchNormalization(name='bn')(x)
    x = Activation('relu', name='relu')(x)
    x = Flatten(name='flatten')(x)
    outputs = Dense(num_classes, activation='softmax', name='output')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model

class Padam(tfa.optimizers.RectifiedAdam):
    def __init__(self, p=0.5, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.p = p
        
    def _resource_apply_dense(self, grad, var, apply_state=None):
        var_dtype = var.dtype.base_dtype
        local_step = tf.cast(self.iterations + 1, var_dtype)
        beta_1_t = tf.cast(self._get_hyper('beta_1', var_dtype), var_dtype)
        beta_2_t = tf.cast(self._get_hyper('beta_2', var_dtype), var_dtype)
        one_minus_beta_1_t = 1 - beta_1_t
        one_minus_beta_2_t = 1 - beta_2_t
        
        step_size = self._get_hyper('learning_rate', var_dtype) * (tf.sqrt(1 - tf.pow(beta_2_t, local_step)) / (1 - tf.pow(beta_1_t, local_step)))

        m = self.get_slot(var, 'm')
        m_t = (m * beta_1_t) + (grad * one_minus_beta_1_t)
        m_hat_t = m_t / (1 - tf.pow(beta_1_t, local_step))
        
        v = self.get_slot(var, 'v')
        v_t = (v * beta_2_t) + (tf.square(grad) * one_minus_beta_2_t)
        v_hat_t = v_t / (1 - tf.pow(beta_2_t, local_step))
        
        denom = tf.pow(tf.sqrt(v_hat_t) + K.epsilon(), self.p)
        var_t = var - step_size * (m_hat_t / denom)
        
        return tf.compat.v1.assign(var, var_t)

"""# Train Model"""

# This function use Resnet 50 architecture
def train_model_with_optimizer(optimizer, batch_size, epochs):
    input_shape = (32, 32, 3)
    num_classes = 10
    model = create_tiny_resnet(input_shape, num_classes)
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy', 'top_k_categorical_accuracy'])
    start_time = time.time()
    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), verbose=0)
    end_time = time.time()
    training_time = end_time - start_time
    return history, training_time, model

# 200 epochs (Same as the paper)
# Need to re-complie the model if ran 50 epochs before
optimizers = {
    'Adam': Adam(learning_rate=0.001),
    'SGD': SGD(learning_rate=0.001, momentum=0.9),
    'Nadam': Nadam(learning_rate=0.001),
    'Adam_Amsgrad': Adam(learning_rate=0.001, amsgrad=True), 
    'Padam': Padam(learning_rate=0.1, total_steps=10000, p=0.125, beta_1=0.9, beta_2=0.999, weight_decay=5e-4)
}

histories_200epochs = {}
training_times_200epochs = {}
models = {}

for name, optimizer in optimizers.items():
    print(f"Training with {name} optimizer...")
    histories_200epochs[name], training_times_200epochs[name], models[name] = train_model_with_optimizer(optimizer, 128, 100)

for name, model in models.items():
    model.save(f'./Model/resnet_model_{name}.h5')

"""# Plot figure without padam"""

plt.figure(figsize=(12, 6))

for name, history in histories_200epochs.items():
  if name != 'Padam':
    plt.plot(history.history['loss'], label=f"{name}")

plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Training Loss Comparison for Different Optimizers')
# Save the figure before displaying it
plt.savefig('./Results/Train_Loss_nopadam.png', dpi=300, bbox_inches='tight')
plt.show()

plt.figure(figsize=(12, 6))

for name, history in histories_200epochs.items():
  if name != 'Padam':
    test_error = [1 - accuracy for accuracy in history.history['val_accuracy']]
    plt.plot(test_error, label=f"{name}", linestyle='--')

plt.xlabel('Epochs')
plt.ylabel('Loss / Error')
plt.legend()
plt.title('Test Error Comparison for Different Optimizers')
# Save the figure before displaying it
plt.savefig('./Results/Test_Error_nopadam.png', dpi=300, bbox_inches='tight')
plt.show()

plt.figure(figsize=(10, 5))
for name, history in histories_200epochs.items():
  if name != 'Padam':
    plt.plot(history.history['val_accuracy'], label=f"{name}")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Validation Accuracy for Different Optimizers')
plt.savefig('./Results/validation_accuracy_nopadam.png', dpi=300, bbox_inches='tight')
plt.show()

plt.figure(figsize=(10, 5))

# Exclude the 'Padam' optimizer
filtered_optimizers = {k: v for k, v in training_times_200epochs.items() if k != 'Padam'}

plt.bar(filtered_optimizers.keys(), filtered_optimizers.values())
plt.xlabel('Optimizer')
plt.ylabel('Time (s)')
plt.title('Training Time for Different Optimizers (excluding Padam)')

plt.savefig('./Results/training_time_nopadam.png', dpi=300, bbox_inches='tight')
plt.show()

fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Training Loss
for name, history in histories_200epochs.items():
  if name != 'Padam':
    axes[0, 0].plot(history.history['loss'], label=f"{name}")
axes[0, 0].set_xlabel('Epochs')
axes[0, 0].set_ylabel('Loss')
axes[0, 0].legend()
axes[0, 0].set_title('Training Loss')

# Test Error
for name, history in histories_200epochs.items():
  if name != 'Padam':
    test_error = [1 - accuracy for accuracy in history.history['val_accuracy']]
    axes[0, 1].plot(test_error, label=f"{name}")
axes[0, 1].set_xlabel('Epochs')
axes[0, 1].set_ylabel('Error')
axes[0, 1].legend()
axes[0, 1].set_title('Test Error')

# Validation Accuracy
for name, history in histories_200epochs.items():
  if name != 'Padam':
    axes[1, 0].plot(history.history['val_accuracy'], label=f"{name}")
axes[1, 0].set_xlabel('Epochs')
axes[1, 0].set_ylabel('Accuracy')
axes[1, 0].legend()
axes[1, 0].set_title('Validation Accuracy')

# Training Time
filtered_optimizers = {k: v for k, v in training_times_200epochs.items() if k != 'Padam'}
axes[1, 1].bar(filtered_optimizers.keys(), filtered_optimizers.values())
axes[1, 1].set_xlabel('Optimizer')
axes[1, 1].set_ylabel('Time (s)')
axes[1, 1].set_title('Training Time')

plt.tight_layout()
plt.savefig('./Results/matrix_nopadam.png', dpi=300, bbox_inches='tight')
plt.show()

"""# Plot figure with padam"""

plt.figure(figsize=(12, 6))

for name, history in histories_200epochs.items():
    plt.plot(history.history['loss'], label=f"{name}")

plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Training Loss Comparison for Different Optimizers')
# Save the figure before displaying it
plt.savefig('./Results/Train_Loss.png', dpi=300, bbox_inches='tight')
plt.show()

plt.figure(figsize=(12, 6))

for name, history in histories_200epochs.items():
    # plt.plot(history.history['loss'], label=f"{name} Training Loss")
    test_error = [1 - accuracy for accuracy in history.history['val_accuracy']]
    plt.plot(test_error, label=f"{name}", linestyle='--')

plt.xlabel('Epochs')
plt.ylabel('Loss / Error')
plt.legend()
plt.title('Test Error Comparison for Different Optimizers')
# Save the figure before displaying it
plt.savefig('./Results/Test_Error.png', dpi=300, bbox_inches='tight')
plt.show()

plt.figure(figsize=(10, 5))
for name, history in histories_200epochs.items():
    plt.plot(history.history['val_accuracy'], label=f"{name}")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Validation Accuracy for Different Optimizers')
plt.savefig('./Results/validation_accuracy.png', dpi=300, bbox_inches='tight')
plt.show()

plt.figure(figsize=(10, 5))
plt.bar(training_times_200epochs.keys(), training_times_200epochs.values())
plt.xlabel('Optimizer')
plt.ylabel('Time (s)')
plt.title('Training Time for Different Optimizers')
plt.savefig('./Results/training_time.png', dpi=300, bbox_inches='tight')
plt.show()

fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Training Loss
for name, history in histories_200epochs.items():
    axes[0, 0].plot(history.history['loss'], label=f"{name}")
axes[0, 0].set_xlabel('Epochs')
axes[0, 0].set_ylabel('Loss')
axes[0, 0].legend()
axes[0, 0].set_title('Training Loss')

# Test Error
for name, history in histories_200epochs.items():
    test_error = [1 - accuracy for accuracy in history.history['val_accuracy']]
    axes[0, 1].plot(test_error, label=f"{name}")
axes[0, 1].set_xlabel('Epochs')
axes[0, 1].set_ylabel('Error')
axes[0, 1].legend()
axes[0, 1].set_title('Test Error')

# Validation Accuracy
for name, history in histories_200epochs.items():
    axes[1, 0].plot(history.history['val_accuracy'], label=f"{name}")
axes[1, 0].set_xlabel('Epochs')
axes[1, 0].set_ylabel('Accuracy')
axes[1, 0].legend()
axes[1, 0].set_title('Validation Accuracy')

# Training Time
axes[1, 1].bar(training_times_200epochs.keys(), training_times_200epochs.values())
axes[1, 1].set_xlabel('Optimizer')
axes[1, 1].set_ylabel('Time (s)')
axes[1, 1].set_title('Training Time')

plt.tight_layout()
plt.savefig('./Results/matrix.png', dpi=300, bbox_inches='tight')
plt.show()

!zip -r Model.zip ./Model
!zip -r Result.zip ./Results
!cp Model.zip /content/drive/MyDrive/TinyResNet/
!cp Result.zip /content/drive/MyDrive/TinyResNet/